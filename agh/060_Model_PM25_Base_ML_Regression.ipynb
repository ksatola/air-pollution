{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "From: https://github.com/ksatola\n",
    "Version: 0.0.5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - PM2.5 - Machine Learning Modelling - Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [Machine Learning Regression](#mlr)\n",
    "- Hourly prediction\n",
    "    - [Load hourly data](#data_h)\n",
    "    - [Base Modelling](#model_h)\n",
    "    - [Hyper-parameters Tuning](#model_h_tune)\n",
    "- Daily prediction\n",
    "    - [Load daily data](#data_d)\n",
    "    - [Modelling](#model_d)\n",
    "    - [Hyper-parameters Tuning](#model_d_tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from model import (\n",
    "    #get_ensemble_models_for_regression,\n",
    "    #get_analytical_view_for_meta_model,\n",
    "    #fit_base_models,\n",
    "    #fit_meta_model,\n",
    "    #evaluate_models,\n",
    "    #predict_with_super_learner\n",
    "#)\n",
    "\n",
    "from model import (\n",
    "    get_pm25_data_for_modelling,\n",
    "    split_df_for_ml_modelling,\n",
    "    get_models_for_regression\n",
    ")\n",
    "\n",
    "from measure import (\n",
    "    #get_rmse\n",
    "    score_ml_models\n",
    ")\n",
    "\n",
    "#from plot import (\n",
    "    #plot_train_test_predicted,\n",
    "#    plot_observed_vs_predicted,\n",
    "#    plot_observations_to_predictions_relationship,\n",
    "    #fit_theoretical_dist_and_plot\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='mlr'></a>\n",
    "\n",
    "## Machine Learning Regression\n",
    "\n",
    "XXXXXXXXX\n",
    "\n",
    "- Linear Algorithms: Logistic Regression.\n",
    "- Nonlinear Algorithms: Classification and Regression Trees (CART), Support Vector Machines (SVM), Gaussian Naive Bayes (NB) and k-Nearest Neighbors (KNN).\n",
    "\n",
    "Napisac z scikit learn wstep do regresji\n",
    "\n",
    "XXXX\n",
    "<img src=\"images/super_learner_algorithm_flow_diagram.png\" style=\"width: 800px;\"/>\n",
    "From https://www.degruyter.com/view/journals/sagmb/6/1/article-sagmb.2007.6.1.1309.xml.xml\n",
    "\n",
    "\n",
    "score models na zbiorze testowym!\n",
    "zrobic final test wybranych modeli ML na zbiorze testowym i policzyc RMSE -> symulacja predykcji\n",
    "jesli linear regression jest najlepsze to sprawdzic OLS assumptions dla zbioru danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='data_h'></a>\n",
    "\n",
    "## Load hourly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "common.py | 42 | get_pm25_data_for_modelling | 03-Jun-20 18:39:37 | INFO: Dataframe loaded: /Users/ksatola/Documents/git/air-polution/agh/data/dfpm25_2008-2018_ml_24hours_lags.hdf\n",
      "common.py | 43 | get_pm25_data_for_modelling | 03-Jun-20 18:39:37 | INFO: Dataframe size: (96378, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>t-1</th>\n",
       "      <th>t-2</th>\n",
       "      <th>t-3</th>\n",
       "      <th>t-4</th>\n",
       "      <th>t-5</th>\n",
       "      <th>t-6</th>\n",
       "      <th>t-7</th>\n",
       "      <th>t-8</th>\n",
       "      <th>t-9</th>\n",
       "      <th>t-10</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>dayofyear</th>\n",
       "      <th>weekofyear</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>quarter</th>\n",
       "      <th>season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-18.246170</td>\n",
       "      <td>-13.443917</td>\n",
       "      <td>0.156504</td>\n",
       "      <td>9.915600</td>\n",
       "      <td>10.136910</td>\n",
       "      <td>13.456166</td>\n",
       "      <td>14.163808</td>\n",
       "      <td>14.019144</td>\n",
       "      <td>20.936157</td>\n",
       "      <td>23.611710</td>\n",
       "      <td>28.537801</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-25.839649</td>\n",
       "      <td>-18.246170</td>\n",
       "      <td>-13.443917</td>\n",
       "      <td>0.156504</td>\n",
       "      <td>9.915600</td>\n",
       "      <td>10.136910</td>\n",
       "      <td>13.456166</td>\n",
       "      <td>14.163808</td>\n",
       "      <td>14.019144</td>\n",
       "      <td>20.936157</td>\n",
       "      <td>23.611710</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-16.761822</td>\n",
       "      <td>-25.839649</td>\n",
       "      <td>-18.246170</td>\n",
       "      <td>-13.443917</td>\n",
       "      <td>0.156504</td>\n",
       "      <td>9.915600</td>\n",
       "      <td>10.136910</td>\n",
       "      <td>13.456166</td>\n",
       "      <td>14.163808</td>\n",
       "      <td>14.019144</td>\n",
       "      <td>20.936157</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-6.697300</td>\n",
       "      <td>-16.761822</td>\n",
       "      <td>-25.839649</td>\n",
       "      <td>-18.246170</td>\n",
       "      <td>-13.443917</td>\n",
       "      <td>0.156504</td>\n",
       "      <td>9.915600</td>\n",
       "      <td>10.136910</td>\n",
       "      <td>13.456166</td>\n",
       "      <td>14.163808</td>\n",
       "      <td>14.019144</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.515568</td>\n",
       "      <td>-6.697300</td>\n",
       "      <td>-16.761822</td>\n",
       "      <td>-25.839649</td>\n",
       "      <td>-18.246170</td>\n",
       "      <td>-13.443917</td>\n",
       "      <td>0.156504</td>\n",
       "      <td>9.915600</td>\n",
       "      <td>10.136910</td>\n",
       "      <td>13.456166</td>\n",
       "      <td>14.163808</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           t        t-1        t-2        t-3        t-4        t-5  \\\n",
       "0 -18.246170 -13.443917   0.156504   9.915600  10.136910  13.456166   \n",
       "1 -25.839649 -18.246170 -13.443917   0.156504   9.915600  10.136910   \n",
       "2 -16.761822 -25.839649 -18.246170 -13.443917   0.156504   9.915600   \n",
       "3  -6.697300 -16.761822 -25.839649 -18.246170 -13.443917   0.156504   \n",
       "4  -2.515568  -6.697300 -16.761822 -25.839649 -18.246170 -13.443917   \n",
       "\n",
       "         t-6        t-7        t-8        t-9       t-10  month  day  hour  \\\n",
       "0  14.163808  14.019144  20.936157  23.611710  28.537801      1    1    11   \n",
       "1  13.456166  14.163808  14.019144  20.936157  23.611710      1    1    12   \n",
       "2  10.136910  13.456166  14.163808  14.019144  20.936157      1    1    13   \n",
       "3   9.915600  10.136910  13.456166  14.163808  14.019144      1    1    14   \n",
       "4   0.156504   9.915600  10.136910  13.456166  14.163808      1    1    15   \n",
       "\n",
       "   dayofyear  weekofyear  dayofweek  quarter  season  \n",
       "0          1           1          1        1       1  \n",
       "1          1           1          1        1       1  \n",
       "2          1           1          1        1       1  \n",
       "3          1           1          1        1       1  \n",
       "4          1           1          1        1       1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfh = get_pm25_data_for_modelling('ml', 'h')\n",
    "dfh.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='model_h'></a>\n",
    "\n",
    "## Base Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = split_df_for_ml_modelling(data=dfh, \n",
    "                                                             target_col='t', \n",
    "                                                             train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('LinearRegression', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False))\n",
      "('ElasticNet', ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
      "           max_iter=1000, normalize=False, positive=False, precompute=False,\n",
      "           random_state=None, selection='cyclic', tol=0.0001, warm_start=False))\n",
      "('SVR', SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='scale',\n",
      "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False))\n",
      "('DecisionTreeRegressor', DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=None,\n",
      "                      max_features=None, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=None, splitter='best'))\n",
      "('KNeighborsRegressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                    metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                    weights='uniform'))\n",
      "('AdaBoostRegressor', AdaBoostRegressor(base_estimator=None, learning_rate=1.0, loss='linear',\n",
      "                  n_estimators=50, random_state=None))\n",
      "('BaggingRegressor', BaggingRegressor(base_estimator=None, bootstrap=True, bootstrap_features=False,\n",
      "                 max_features=1.0, max_samples=1.0, n_estimators=10,\n",
      "                 n_jobs=None, oob_score=False, random_state=None, verbose=0,\n",
      "                 warm_start=False))\n",
      "('RandomForestRegressor', RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=10, n_jobs=None, oob_score=False,\n",
      "                      random_state=None, verbose=0, warm_start=False))\n",
      "('ExtraTreesRegressor', ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
      "                    max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                    max_samples=None, min_impurity_decrease=0.0,\n",
      "                    min_impurity_split=None, min_samples_leaf=1,\n",
      "                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                    n_estimators=10, n_jobs=None, oob_score=False,\n",
      "                    random_state=None, verbose=0, warm_start=False))\n"
     ]
    }
   ],
   "source": [
    "# Define regression models in scope\n",
    "reg_models = get_models_for_regression()\n",
    "models = []\n",
    "\n",
    "for model in reg_models:\n",
    "    item = (type(model).__name__, model)\n",
    "    models.append(item)\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression, RMSE 6.65957411419811, (std. dev. 0.24797679013828025)\n",
      "ElasticNet, RMSE 6.661554475363767, (std. dev. 0.25693123328598905)\n",
      "SVR, RMSE 10.233044938426854, (std. dev. 0.4100088165603076)\n",
      "DecisionTreeRegressor, RMSE 9.456092893179248, (std. dev. 0.20927662240175304)\n",
      "KNeighborsRegressor, RMSE 9.217966522916125, (std. dev. 0.364490621665717)\n",
      "AdaBoostRegressor, RMSE 9.232439669157156, (std. dev. 0.4034306021382163)\n",
      "BaggingRegressor, RMSE 7.078077379118675, (std. dev. 0.1507370826850066)\n",
      "RandomForestRegressor, RMSE 7.0144764807240945, (std. dev. 0.17301072119758656)\n",
      "ExtraTreesRegressor, RMSE 7.0341519710071, (std. dev. 0.23452597614384532)\n",
      "CPU times: user 1min 18s, sys: 1.8 s, total: 1min 20s\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Perform initial ranking\n",
    "scores, results, names = score_ml_models(X_train=X_train,\n",
    "                                         y_train=y_train,\n",
    "                                         models=models,\n",
    "                                         n_splits = 5,\n",
    "                                         metric='neg_root_mean_squared_error',\n",
    "                                         metric_label=\"RMSE\", \n",
    "                                         seed=123)\n",
    "for score in scores:\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare Algorithms\n",
    "fig = plt.figure(figsize=(18, 8))\n",
    "fig.suptitle('Algorithms Comparison - RMSE')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='model_h_tune'></a>\n",
    "\n",
    "## Hyper-parameter Tuning\n",
    "\n",
    "`Hyper-parameters` are parameters that are not directly learnt within estimators. It is possible and recommended to search the hyper-parameter space **for the best cross validation score**.\n",
    "\n",
    "While using a grid of parameter settings is currently the most widely used method for parameter optimization, other search methods have more favourable properties. RandomizedSearchCV implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values. This has two main benefits over an exhaustive search:\n",
    "\n",
    "A budget can be chosen independent of the number of parameters and possible values.\n",
    "\n",
    "Adding parameters that do not influence the performance does not decrease efficiency.\n",
    "\n",
    "Specifying how parameters should be sampled is done using a dictionary, very similar to specifying parameters for GridSearchCV. Additionally, a computation budget, being the number of sampled candidates or sampling iterations, is specified using the n_iter parameter. For each parameter, either a distribution over possible values or a list of discrete choices (which will be sampled uniformly) can be specified:\n",
    "\n",
    "loguniform(1, 100) can be used instead of [1, 10, 100] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_grid_search_cv2(X_train, y_train, model, param_grid, scoring, num_folds = 6, seed = 123):\n",
    "\n",
    "    kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
    "    grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "    means = grid_result.cv_results_['mean_test_score'] \n",
    "    stds = grid_result.cv_results_['std_test_score'] \n",
    "    params = grid_result.cv_results_['params']\n",
    "\n",
    "    #for mean, stdev, param in zip(means, stds, params):\n",
    "        #print(\"{:0.2f} ({:0.2f}) with: {}\".format(mean, stdev, param))\n",
    "    #print('-------')\n",
    "    print(f'Best: {grid_result.best_score_} using {grid_result.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/hyperparameter-tuning-c5619e7e6624\n",
    "def perform_random_search_cv2(X_train, y_train, model, param_grid, scoring, num_folds = 6, seed = 123):\n",
    "\n",
    "    kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "    grid = RandomizedSearchCV(estimator=model,\n",
    "                            param_distributions=param_grid,\n",
    "                            scoring=scoring,\n",
    "                            verbose=1, \n",
    "                            n_jobs=-1,\n",
    "                            n_iter=1000)\n",
    "    grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "    means = grid_result.cv_results_['mean_test_score'] \n",
    "    stds = grid_result.cv_results_['std_test_score'] \n",
    "    params = grid_result.cv_results_['params']\n",
    "\n",
    "    #for mean, stdev, param in zip(means, stds, params):\n",
    "        #print(\"{:0.2f} ({:0.2f}) with: {}\".format(mean, stdev, param))\n",
    "    #print('-------')\n",
    "    print(f'Best: {grid_result.best_score_} using {grid_result.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#LinearRegression\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html\n",
    "# Linear regression with combined L1 and L2 priors as regularizer.\n",
    "param_grid = {\n",
    "    \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    \"l1_ratio\": np.arange(0.0, 1.0, 0.1),\n",
    "    \"max_iter\": [1, 10, 100, 1000],\n",
    "    \"fit_intercept\": [True, False],\n",
    "    \"selection\": [\"cyclic\", \"random\"]\n",
    "}\n",
    "model = models[1][1]\n",
    "perform_grid_search_cv2(X_train=X_train, \n",
    "                        y_train=y_train, \n",
    "                        model=model, \n",
    "                        param_grid=param_grid, \n",
    "                        scoring='neg_root_mean_squared_error', \n",
    "                        num_folds=6, \n",
    "                        seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -6.656139464878051 using {'alpha': 0.1, 'fit_intercept': True, 'l1_ratio': 0.9, 'max_iter': 100, 'selection': 'cyclic'}\n",
      "CPU times: user 20min 15s, sys: 8min 36s, total: 28min 52s\n",
      "Wall time: 5min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#ElasticNet\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html\n",
    "# Linear regression with combined L1 and L2 priors as regularizer.\n",
    "param_grid = {\n",
    "    \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    \"l1_ratio\": np.arange(0.0, 1.0, 0.1),\n",
    "    \"max_iter\": [1, 10, 100, 1000],\n",
    "    \"fit_intercept\": [True, False],\n",
    "    \"selection\": [\"cyclic\", \"random\"]\n",
    "}\n",
    "model = models[1][1]\n",
    "perform_grid_search_cv2(X_train=X_train, \n",
    "                        y_train=y_train, \n",
    "                        model=model, \n",
    "                        param_grid=param_grid, \n",
    "                        scoring='neg_root_mean_squared_error', \n",
    "                        num_folds=6, \n",
    "                        seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best: -6.656139464878051 using {'alpha': 0.1, 'fit_intercept': True, 'l1_ratio': 0.9, 'max_iter': 100, 'selection': 'cyclic'}\n",
    "CPU times: user 20min 15s, sys: 8min 36s, total: 28min 52s\n",
    "Wall time: 5min 40s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 800 candidates, totalling 4000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 296 tasks      | elapsed:   10.1s\n",
      "[Parallel(n_jobs=-1)]: Done 796 tasks      | elapsed:   25.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1496 tasks      | elapsed:   46.1s\n",
      "[Parallel(n_jobs=-1)]: Done 2396 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 3496 tasks      | elapsed:  1.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -6.657569882643266 using {'selection': 'cyclic', 'max_iter': 100, 'l1_ratio': 0.9, 'fit_intercept': True, 'alpha': 0.1}\n",
      "CPU times: user 16.7 s, sys: 1.48 s, total: 18.2 s\n",
      "Wall time: 1min 35s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 4000 out of 4000 | elapsed:  1.6min finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#ElasticNet\n",
    "perform_random_search_cv2(X_train=X_train, \n",
    "                        y_train=y_train, \n",
    "                        model=model, \n",
    "                        param_grid=param_grid, \n",
    "                        scoring='neg_root_mean_squared_error', \n",
    "                        num_folds=6, \n",
    "                        seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fitting 5 folds for each of 800 candidates, totalling 4000 fits\n",
    "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
    "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.0s\n",
    "[Parallel(n_jobs=-1)]: Done 296 tasks      | elapsed:   10.1s\n",
    "[Parallel(n_jobs=-1)]: Done 796 tasks      | elapsed:   25.3s\n",
    "[Parallel(n_jobs=-1)]: Done 1496 tasks      | elapsed:   46.1s\n",
    "[Parallel(n_jobs=-1)]: Done 2396 tasks      | elapsed:  1.1min\n",
    "[Parallel(n_jobs=-1)]: Done 3496 tasks      | elapsed:  1.5min\n",
    "Best: -6.657569882643266 using {'selection': 'cyclic', 'max_iter': 100, 'l1_ratio': 0.9, 'fit_intercept': True, 'alpha': 0.1}\n",
    "CPU times: user 16.7 s, sys: 1.48 s, total: 18.2 s\n",
    "Wall time: 1min 35s\n",
    "[Parallel(n_jobs=-1)]: Done 4000 out of 4000 | elapsed:  1.6min finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#SVR\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html\n",
    "# Epsilon-Support Vector Regression. \n",
    "# The method of Support Vector Classification can be extended to solve regression problems. This method is called Support Vector Regression.\n",
    "# The model produced by support vector classification (as described above) depends only on a subset of the training data, because the cost function for building the model does not care about training points that lie beyond the margin. Analogously, the model produced by Support Vector Regression depends only on a subset of the training data, because the cost function ignores samples whose prediction is close to their target.\n",
    "param_grid = {\n",
    "    \"kernel\": (\"linear\", \"rbf\", \"poly\", \"sigmoid\"), \n",
    "    \"degree\": [1, 2, 3, 4],\n",
    "    \"C\": [1.5, 10],\n",
    "    \"gamma\": [\"scale\", \"auto\"],\n",
    "    'epsilon':[0.01, 0.1, 0.2, 0.3, 0.5]\n",
    "}\n",
    "model = models[2][1]\n",
    "perform_grid_search_cv2(X_train=X_train, \n",
    "                        y_train=y_train, \n",
    "                        model=model, \n",
    "                        param_grid=param_grid, \n",
    "                        scoring='neg_root_mean_squared_error', \n",
    "                        num_folds=6, \n",
    "                        seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#SVR\n",
    "perform_random_search_cv2(X_train=X_train, \n",
    "                        y_train=y_train, \n",
    "                        model=model, \n",
    "                        param_grid=param_grid, \n",
    "                        scoring='neg_root_mean_squared_error', \n",
    "                        num_folds=6, \n",
    "                        seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#DecisionTreeRegressor\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n",
    "# Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n",
    "param_grid = {\n",
    "    \"criterion\": [\"mse\", \"friedman_mse\", \"mae\"],\n",
    "    \"splitter\": [\"best\", \"random\"],\n",
    "    \"max_depth\": range(2, 16, 2),\n",
    "    \"min_samples_split\": range(2, 16, 2),\n",
    "    \"min_samples_leaf\": range(2, 16, 2),\n",
    "    \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "    \"ccp_alpha\": [0.001, 0.01, 0.1, 0, 1, 10, 100, 1000]\n",
    "}\n",
    "model = models[3][1]\n",
    "perform_grid_search_cv2(X_train=X_train, \n",
    "                        y_train=y_train, \n",
    "                        model=model, \n",
    "                        param_grid=param_grid, \n",
    "                        scoring='neg_root_mean_squared_error', \n",
    "                        num_folds=6, \n",
    "                        seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#BaggingRegressor\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html\n",
    "# https://www.programcreek.com/python/example/85938/sklearn.ensemble.BaggingRegressor\n",
    "# A Bagging regressor is an ensemble meta-estimator that fits base regressors each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.\n",
    "# The base estimator to fit on random subsets of the dataset. If None, then the base estimator is a decision tree.\n",
    "param_grid = {\n",
    "    \"base_estimator\": [models[0][1]]\n",
    "    \"n_estimators\": [100, 200, 500, 1000, 5000, 10000],\n",
    "    \"bootstrap\": [True, False],\n",
    "    \"min_samples_split\": range(2, 16, 2),\n",
    "    \"bootstrap\": [True, False],\n",
    "    \"n_jobs\": [-1]\n",
    "}\n",
    "model = models[6][1]\n",
    "perform_grid_search_cv2(X_train=X_train, \n",
    "                        y_train=y_train, \n",
    "                        model=model, \n",
    "                        param_grid=param_grid, \n",
    "                        scoring='neg_root_mean_squared_error', \n",
    "                        num_folds=6, \n",
    "                        seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#RandomForestRegressor\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "# https://www.programcreek.com/python/example/85938/sklearn.ensemble.BaggingRegressor\n",
    "# A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree.\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200, 500, 1000, 5000, 10000],\n",
    "    \"criterion\": [\"mse\", \"mae\"],\n",
    "    \"bootstrap\": [True, False],\n",
    "    \"ccp_alpha\": [0.001, 0.01, 0.1, 0, 1, 10, 100, 1000],\n",
    "    \"max_depth\": [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "    \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"min_samples_split\": range(2, 16, 2),\n",
    "    \"n_jobs\": [-1]\n",
    "}\n",
    "model = models[7][1]\n",
    "perform_grid_search_cv2(X_train=X_train, \n",
    "                        y_train=y_train, \n",
    "                        model=model, \n",
    "                        param_grid=param_grid, \n",
    "                        scoring='neg_root_mean_squared_error', \n",
    "                        num_folds=6, \n",
    "                        seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#ExtraTreesRegressor\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html\n",
    "# https://www.programcreek.com/python/example/102434/sklearn.ensemble.ExtraTreesRegressor\n",
    "# This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200, 500, 1000, 5000, 10000],\n",
    "    \"criterion\": [\"mse\", \"mae\"],\n",
    "    \"bootstrap\": [True, False],\n",
    "    \"ccp_alpha\": [0.001, 0.01, 0.1, 0, 1, 10, 100, 1000],\n",
    "    \"max_depth\": [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "    \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"min_samples_split\": range(2, 16, 2),\n",
    "    \"n_jobs\": [-1]\n",
    "}\n",
    "model = models[8][1]\n",
    "perform_grid_search_cv2(X_train=X_train, \n",
    "                        y_train=y_train, \n",
    "                        model=model, \n",
    "                        param_grid=param_grid, \n",
    "                        scoring='neg_root_mean_squared_error', \n",
    "                        num_folds=6, \n",
    "                        seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='data_d'></a>\n",
    "\n",
    "## Load daily data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd = get_pm25_data_for_modelling('ml', 'd')\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='model_d'></a>\n",
    "\n",
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='model_d_tune'></a>\n",
    "\n",
    "## Hyper-parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PRZEGLADNAC TO CO PONIZEJ I USUNAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "https://scikit-learn.org/stable/model_selection.html\n",
    "\n",
    "https://pythonprogramming.net/forecasting-predicting-machine-learning-tutorial/\n",
    "\n",
    "XGBoost: https://machinelearningmastery.com/calculate-feature-importance-with-python/\n",
    "\n",
    "http://queirozf.com/entries/scikit-learn-pipeline-examples\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "- [Decision Tree](#tree)\n",
    "- Hourly prediction\n",
    "    - [Load hourly data](#data_h)\n",
    "    - [Modelling](#model_h)\n",
    "- Daily prediction\n",
    "    - [Load daily data](#data_d)\n",
    "    - [Modelling](#model_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search: hyper-parameters tuning\n",
    "https://scikit-learn.org/stable/modules/grid_search.html#grid-search\n",
    "\n",
    "Hyper-parameters are parameters that are not directly learnt within estimators. In scikit-learn they are passed as arguments to the constructor of the estimator classes. Typical examples include C, kernel and gamma for Support Vector Classifier, alpha for Lasso, etc.\n",
    "\n",
    "Some models allow for specialized, efficient parameter search strategies, outlined below. Two generic approaches to sampling search candidates are provided in scikit-learn: for given values, GridSearchCV exhaustively considers all parameter combinations, while RandomizedSearchCV can sample a given number of candidates from a parameter space with a specified distribution.\n",
    "\n",
    "Note that it is common that a small subset of those parameters can have a large impact on the predictive or computation performance of the model while others can be left to their default values. It is recommended to read the docstring of the estimator class to get a finer understanding of their expected behavior, possibly by reading the enclosed reference to the literature.\n",
    "\n",
    "While using a grid of parameter settings is currently the most widely used method for parameter optimization, other search methods have more favourable properties. RandomizedSearchCV implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values. This has two main benefits over an exhaustive search:\n",
    "\n",
    "- A budget can be chosen independent of the number of parameters and possible values.\n",
    "\n",
    "- Adding parameters that do not influence the performance does not decrease efficiency.\n",
    "\n",
    "## Cross-validation: evaluating estimator performance\n",
    "https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "\n",
    "Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called overfitting. To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a test set X_test, y_test.\n",
    "\n",
    "When evaluating different settings (“hyperparameters”) for estimators, such as the C setting that must be manually set for an SVM, there is still a risk of overfitting on the test set because the parameters can be tweaked until the estimator performs optimally. This way, knowledge about the test set can “leak” into the model and evaluation metrics no longer report on generalization performance. To solve this problem, yet another part of the dataset can be held out as a so-called “validation set”: training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set.\n",
    "\n",
    "However, by partitioning the available data into three sets, we drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets.\n",
    "\n",
    "A solution to this problem is a procedure called cross-validation (CV for short). A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. In the basic approach, called k-fold CV, the training set is split into k smaller sets (other approaches are described below, but generally follow the same principles). The following procedure is followed for each of the k “folds”:\n",
    "\n",
    "- A model is trained using  of the folds as training data;\n",
    "\n",
    "- the resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).\n",
    "\n",
    "The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop. This approach can be computationally expensive, but does not waste too much data (as is the case when fixing an arbitrary validation set), which is a major advantage in problems such as inverse inference where the number of samples is very small.\n",
    "\n",
    "dac obrazek ze strony, zrobic source: sklearn\n",
    "\n",
    "## Pipelines and composite estimators\n",
    "https://scikit-learn.org/stable/modules/compose.html\n",
    "\n",
    "Transformers are usually combined with classifiers, regressors or other estimators to build a composite estimator. The most common tool is a Pipeline.\n",
    "\n",
    "Pipeline can be used to chain multiple estimators into one. This is useful as there is often a fixed sequence of steps in processing the data, for example feature selection, normalization and classification. Pipeline serves multiple purposes here:\n",
    "\n",
    "Convenience and encapsulation\n",
    "You only have to call fit and predict once on your data to fit a whole sequence of estimators.\n",
    "\n",
    "Joint parameter selection\n",
    "You can grid search over parameters of all estimators in the pipeline at once.\n",
    "\n",
    "Safety\n",
    "Pipelines help avoid leaking statistics from your test data into the trained model in cross-validation, by ensuring that the same samples are used to train the transformers and predictors.\n",
    "\n",
    "All estimators in a pipeline, except the last one, must be transformers (i.e. must have a transform method). The last estimator may be any type (transformer, classifier, etc.).\n",
    "\n",
    "## Model persistence\n",
    "https://scikit-learn.org/stable/modules/model_persistence.html\n",
    "\n",
    "After training a scikit-learn model, it is desirable to have a way to persist the model for future use without having to retrain. The following section gives you an example of how to persist a model with pickle. \n",
    "\n",
    "pickle (and joblib by extension), has some issues regarding maintainability and security. Because of this,\n",
    "\n",
    "- Never unpickle untrusted data as it could lead to malicious code being executed upon loading.\n",
    "\n",
    "- While models saved using one version of scikit-learn might load in other versions, this is entirely unsupported and inadvisable. It should also be kept in mind that operations performed on such data could give different and unexpected results.\n",
    "\n",
    "In order to rebuild a similar model with future versions of scikit-learn, additional metadata should be saved along the pickled model:\n",
    "\n",
    "- The training data, e.g. a reference to an immutable snapshot\n",
    "\n",
    "- The python source code used to generate the model\n",
    "\n",
    "- The versions of scikit-learn and its dependencies\n",
    "\n",
    "- The cross validation score obtained on the training data\n",
    "\n",
    "- This should make it possible to check that the cross-validation score is in the same range as before.\n",
    "\n",
    "Since a model internal representation may be different on two different architectures, dumping a model on one architecture and loading it on another architecture is not supported.\n",
    "\n",
    "more: https://pyvideo.org/pycon-us-2014/pickles-are-for-delis-not-software.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from model import (\n",
    "#    get_best_arima_params_for_time_series\n",
    "#)\n",
    "\n",
    "from measure import (\n",
    "    get_rmse\n",
    ")\n",
    "\n",
    "from plot import (\n",
    "    #plot_train_test_predicted,\n",
    "    plot_observed_vs_predicted,\n",
    "    plot_observations_to_predictions_relationship,\n",
    "    #fit_theoretical_dist_and_plot\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Users/ksatola/Documents/git/air-polution/data/final/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='data_h'></a>\n",
    "\n",
    "## Load hourly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_hdf = data_path + 'dfpm25_2008-2018_ml_24hours_lags.hdf'\n",
    "\n",
    "df = pd.read_hdf(path_or_buf=data_file_hdf, key=\"df\")\n",
    "print(f'Dataframe size: {df.shape}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shorten series for faster fitting\n",
    "#df = df[-(2*365*24):]\n",
    "#df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='model_h'></a>\n",
    "\n",
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into independent variables dataset columns and dependent variable column\n",
    "X = df.iloc[:, 1:]\n",
    "y = df.iloc[:, :1]\n",
    "#y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'X_train: {X_train.shape}')\n",
    "print(f'X_test:  {X_test.shape}')\n",
    "print(f'y_train: {y_train.shape}')\n",
    "print(f'y_test:  {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = tree.DecisionTreeRegressor()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "coefficient of determination R^2\n",
    "\n",
    "The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
    "sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
    "sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
    "The best possible score is 1.0 and it can be negative (because the\n",
    "model can be arbitrarily worse). A constant model that always\n",
    "predicts the expected value of y, disregarding the input features,\n",
    "would get a R^2 score of 0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Check the score on train and test\n",
    "print(model.score(X_train, y_train))\n",
    "print(model.score(X_test, y_test))  # predictions are horrible if negative value, no relationship if 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Cross-validation\n",
    "model = ensemble.GradientBoostingRegressor()\n",
    "scores = cross_val_score(model, X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R2: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the score computed at each CV iteration is the score method of the estimator. It is possible to change this by using the scoring parameter:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn import metrics\n",
    "scores = cross_val_score(model, X_train, y_train, cv=5, scoring='explained_variance')\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_transformed = scaler.transform(X_train)\n",
    "model = ensemble.GradientBoostingRegressor().fit(X_train_transformed, y_train)\n",
    "X_test_transformed = scaler.transform(X_test)\n",
    "model.score(X_test_transformed, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "scoring = ['r2', 'explained_variance', 'max_error']\n",
    "scores = cross_validate(model, X_train, y_train, cv=5, scoring=scoring)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(scores.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores['test_max_error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the names and current values for all parameters for a given estimator\n",
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.fixes import loguniform\n",
    "{'C': loguniform(1e0, 1e3),\n",
    " 'gamma': loguniform(1e-4, 1e-3),\n",
    " 'kernel': ['rbf'],\n",
    " 'class_weight':['balanced', None]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs=-1\n",
    "error_score=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not recommended\n",
    "import pickle\n",
    "s = pickle.dumps(model)\n",
    "#print(s)\n",
    "model2 = pickle.loads(s)\n",
    "model2.predict(X[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommended\n",
    "from joblib import dump, load\n",
    "dump(model, 'model.joblib') \n",
    "model2 = load('model.joblib')\n",
    "model2.predict(X[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "train_sizes, train_scores, valid_scores = learning_curve(ensemble.GradientBoostingRegressor(), \n",
    "                                                         X_train, y_train, \n",
    "                                                         train_sizes=[50, 80, 110], \n",
    "                                                         cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/learning_curve.html#validation-curve\n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html#sphx-glr-auto-examples-model-selection-plot-learning-curve-py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate 3 plots: the test and training learning curve, the training\n",
    "    samples vs fit times curve, the fit times vs score curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    axes : array of 3 axes, optional (default=None)\n",
    "        Axes to use for plotting the curves.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "\n",
    "          - None, to use the default 5-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - :term:`CV splitter`,\n",
    "          - An iterable yielding (train, test) splits as arrays of indices.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : int or None, optional (default=None)\n",
    "        Number of jobs to run in parallel.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
    "        Relative or absolute numbers of training examples that will be used to\n",
    "        generate the learning curve. If the dtype is float, it is regarded as a\n",
    "        fraction of the maximum size of the training set (that is determined\n",
    "        by the selected validation method), i.e. it has to be within (0, 1].\n",
    "        Otherwise it is interpreted as absolute sizes of the training sets.\n",
    "        Note that for classification the number of samples usually have to\n",
    "        be big enough to contain at least one sample from each class.\n",
    "        (default: np.linspace(0.1, 1.0, 5))\n",
    "    \"\"\"\n",
    "    if axes is None:\n",
    "        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "    axes[0].set_title(title)\n",
    "    if ylim is not None:\n",
    "        axes[0].set_ylim(*ylim)\n",
    "    axes[0].set_xlabel(\"Training examples\")\n",
    "    axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = \\\n",
    "        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
    "                       train_sizes=train_sizes,\n",
    "                       return_times=True)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    fit_times_std = np.std(fit_times, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes[0].grid()\n",
    "    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                         color=\"r\")\n",
    "    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
    "                         color=\"g\")\n",
    "    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "                 label=\"Training score\")\n",
    "    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "                 label=\"Cross-validation score\")\n",
    "    axes[0].legend(loc=\"best\")\n",
    "\n",
    "    # Plot n_samples vs fit_times\n",
    "    axes[1].grid()\n",
    "    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n",
    "    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n",
    "                         fit_times_mean + fit_times_std, alpha=0.1)\n",
    "    axes[1].set_xlabel(\"Training examples\")\n",
    "    axes[1].set_ylabel(\"fit_times\")\n",
    "    axes[1].set_title(\"Scalability of the model\")\n",
    "\n",
    "    # Plot fit_time vs score\n",
    "    axes[2].grid()\n",
    "    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n",
    "    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1)\n",
    "    axes[2].set_xlabel(\"fit_times\")\n",
    "    axes[2].set_ylabel(\"Score\")\n",
    "    axes[2].set_title(\"Performance of the model\")\n",
    "\n",
    "    return plt\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(10, 15))\n",
    "\n",
    "#X, y = load_digits(return_X_y=True)\n",
    "\n",
    "title = \"Learning Curves (Naive Bayes)\"\n",
    "# Cross validation with 100 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "#cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n",
    "\n",
    "estimator = ensemble.GradientBoostingRegressor()#GaussianNB()\n",
    "plot_learning_curve(estimator, title, X, y, axes=axes[:, 0], ylim=(0.7, 1.01),\n",
    "                    cv=5, n_jobs=4)\n",
    "\n",
    "title = r\"Learning Curves (SVM, RBF kernel, $\\gamma=0.001$)\"\n",
    "# SVC is more expensive so we do a lower number of CV iterations:\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "estimator = SVC(gamma=0.001)\n",
    "plot_learning_curve(estimator, title, X, y, axes=axes[:, 1], ylim=(0.7, 1.01),\n",
    "                    cv=cv, n_jobs=4)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = get_rmse(y_test, y_pred)\n",
    "print(f'Decision Tree RMSE: {rmse:.4f}')\n",
    "#print(f'Naive forecast correlation coefficient of the observed-to-predicted values percent change: {r:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted against actual values\n",
    "\n",
    "#from matplotlib import pyplot as plt\n",
    "\n",
    "# Use the best max_depth \n",
    "model = tree.DecisionTreeRegressor(max_depth=8)  # fill in best max depth here\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict values for train and test\n",
    "train_prediction = model.predict(X_train)\n",
    "test_prediction = model.predict(X_test)\n",
    "\n",
    "# Scatter the predictions vs actual values\n",
    "plt.scatter(train_prediction, y_train, label='train')  # blue\n",
    "plt.scatter(test_prediction, y_test, label='test')  # orange\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n",
    "# https://machinelearningmastery.com/calculate-feature-importance-with-python/\n",
    "\n",
    "# get importance\n",
    "importance = model.feature_importances_\n",
    "\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "\n",
    "# plot feature importance\n",
    "plt.bar([x for x in range(len(importance))], importance)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.plot_tree(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([X_train.iloc[100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.iloc[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/visualizing-decision-trees-with-python-scikit-learn-graphviz-matplotlib-1c50b4aa68dc\n",
    "\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = StringIO()\n",
    "export_graphviz(model, out_file=dot_data,  \n",
    "                feature_names = X_train.columns,\n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='data_d'></a>\n",
    "\n",
    "## Load daily data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = data_path + 'dfpm25_2008-2018_ml_7days_lags.hdf'\n",
    "\n",
    "df = pd.read_hdf(path_or_buf=data_file, key=\"df\")\n",
    "print(f'Dataframe size: {df.shape}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='model_d'></a>\n",
    "\n",
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into independent variables dataset columns and dependent variable column\n",
    "X = df.iloc[:, 1:]\n",
    "y = df.iloc[:, :1]\n",
    "#y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'X_train: {X_train.shape}')\n",
    "print(f'X_test:  {X_test.shape}')\n",
    "print(f'y_train: {y_train.shape}')\n",
    "print(f'y_test:  {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = tree.DecisionTreeRegressor()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Find the best Max Depth\n",
    "\n",
    "# Loop through a few different max depths and check the performance\n",
    "# Try different max depths. We want to optimize our ML models to make the best predictions possible.\n",
    "# For regular decision trees, max_depth, which is a hyperparameter, limits the number of splits in a tree.\n",
    "# You can find the best value of max_depth based on the R-squared score of the model on the test set.\n",
    "\n",
    "for d in [2, 3, 4, 5,7,8,10]:\n",
    "    # Create the tree and fit it\n",
    "    model = tree.DecisionTreeRegressor(max_depth=d)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = get_rmse(y_test, y_pred)\n",
    "\n",
    "    # Print out the scores on train and test\n",
    "    print('max_depth=', str(d))\n",
    "    print(model.score(X_train, y_train))\n",
    "    print(model.score(X_test, y_test))  # You want the test score to be positive and high\n",
    "    print(f'Decision Tree RMSE: {rmse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted against actual values\n",
    "\n",
    "#from matplotlib import pyplot as plt\n",
    "\n",
    "# Use the best max_depth \n",
    "model = tree.DecisionTreeRegressor(max_depth=3)  # fill in best max depth here\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict values for train and test\n",
    "train_prediction = model.predict(X_train)\n",
    "test_prediction = model.predict(X_test)\n",
    "\n",
    "# Scatter the predictions vs actual values\n",
    "plt.scatter(train_prediction, y_train, label='train')  # blue\n",
    "plt.scatter(test_prediction, y_test, label='test')  # orange\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n",
    "# https://machinelearningmastery.com/calculate-feature-importance-with-python/\n",
    "\n",
    "# get importance\n",
    "importance = model.feature_importances_\n",
    "\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "\n",
    "# plot feature importance\n",
    "plt.bar([x for x in range(len(importance))], importance)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.plot_tree(model, feature_names=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([X_train.iloc[100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.iloc[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = StringIO()\n",
    "export_graphviz(model, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
