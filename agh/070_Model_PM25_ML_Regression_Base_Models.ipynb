{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "From: https://github.com/ksatola\n",
    "Version: 0.1.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - PM2.5 - Machine Learning Modelling - Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [Machine Learning Regression](#mlr)\n",
    "- Hourly prediction\n",
    "    - [Load hourly data](#data_h)\n",
    "    - [Base Modelling](#model_h)\n",
    "    - [Hyper-parameters Tuning](#model_h_tune)\n",
    "- Daily prediction\n",
    "    - [Load daily data](#data_d)\n",
    "    - [Modelling](#model_d)\n",
    "    - [Hyper-parameters Tuning](#model_d_tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.base import BaseEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import (\n",
    "    get_pm25_data_for_modelling,\n",
    "    get_models_for_regression,\n",
    "    split_df_for_ml_modelling_offset,\n",
    "    perform_grid_search_cv,\n",
    "    perform_random_search_cv\n",
    ")\n",
    "\n",
    "from measure import (\n",
    "    get_mean_folds_rmse_for_n_prediction_points,\n",
    "    score_ml_models,\n",
    "    prepare_data_for_visualization,\n",
    "    walk_forward_ml_model_validation\n",
    ")\n",
    "\n",
    "from plot import (\n",
    "   visualize_results\n",
    ")\n",
    "\n",
    "from utils import (\n",
    "    get_datetime_identifier\n",
    ")\n",
    "\n",
    "from logger import logger\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='mlr'></a>\n",
    "\n",
    "## Machine Learning Regression\n",
    "\n",
    "XXXXXXXXX\n",
    "\n",
    "- Linear Algorithms: Logistic Regression.\n",
    "- Nonlinear Algorithms: Classification and Regression Trees (CART), Support Vector Machines (SVM), Gaussian Naive Bayes (NB) and k-Nearest Neighbors (KNN).\n",
    "\n",
    "Napisac z scikit learn wstep do regresji\n",
    "\n",
    "wypisac funkcje - zob. ponizej\n",
    "\n",
    "Hyper based on CV and training data\n",
    "Final check based on test data - TODO\n",
    "\n",
    "w pracy opisac generalne podejscie do ML, proste, dla najlepszych hyper, i dla nich super learner\n",
    "\n",
    "XXXX\n",
    "<img src=\"images/super_learner_algorithm_flow_diagram.png\" style=\"width: 800px;\"/>\n",
    "From https://www.degruyter.com/view/journals/sagmb/6/1/article-sagmb.2007.6.1.1309.xml.xml\n",
    "\n",
    "\n",
    "score models na zbiorze testowym!\n",
    "zrobic final test wybranych modeli ML na zbiorze testowym i policzyc RMSE -> symulacja predykcji\n",
    "jesli linear regression jest najlepsze to sprawdzic OLS assumptions dla zbioru danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='data_h'></a>\n",
    "\n",
    "## Load hourly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "common.py | 42 | get_pm25_data_for_modelling | 12-Jun-20 22:13:04 | INFO: Dataframe loaded: /Users/ksatola/Documents/git/air-pollution/agh/data/dfpm25_2008-2018_ml_24hours_lags.hdf\n",
      "common.py | 43 | get_pm25_data_for_modelling | 12-Jun-20 22:13:04 | INFO: Dataframe size: (96378, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>t-1</th>\n",
       "      <th>t-2</th>\n",
       "      <th>t-3</th>\n",
       "      <th>t-4</th>\n",
       "      <th>t-5</th>\n",
       "      <th>t-6</th>\n",
       "      <th>t-7</th>\n",
       "      <th>t-8</th>\n",
       "      <th>t-9</th>\n",
       "      <th>t-10</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>dayofyear</th>\n",
       "      <th>weekofyear</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>quarter</th>\n",
       "      <th>season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30.5</td>\n",
       "      <td>41.5</td>\n",
       "      <td>62.5</td>\n",
       "      <td>70.5</td>\n",
       "      <td>69.5</td>\n",
       "      <td>67.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>60.5</td>\n",
       "      <td>73.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26.0</td>\n",
       "      <td>30.5</td>\n",
       "      <td>41.5</td>\n",
       "      <td>62.5</td>\n",
       "      <td>70.5</td>\n",
       "      <td>69.5</td>\n",
       "      <td>67.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>60.5</td>\n",
       "      <td>73.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28.5</td>\n",
       "      <td>26.0</td>\n",
       "      <td>30.5</td>\n",
       "      <td>41.5</td>\n",
       "      <td>62.5</td>\n",
       "      <td>70.5</td>\n",
       "      <td>69.5</td>\n",
       "      <td>67.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>60.5</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28.0</td>\n",
       "      <td>28.5</td>\n",
       "      <td>26.0</td>\n",
       "      <td>30.5</td>\n",
       "      <td>41.5</td>\n",
       "      <td>62.5</td>\n",
       "      <td>70.5</td>\n",
       "      <td>69.5</td>\n",
       "      <td>67.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>60.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32.5</td>\n",
       "      <td>28.0</td>\n",
       "      <td>28.5</td>\n",
       "      <td>26.0</td>\n",
       "      <td>30.5</td>\n",
       "      <td>41.5</td>\n",
       "      <td>62.5</td>\n",
       "      <td>70.5</td>\n",
       "      <td>69.5</td>\n",
       "      <td>67.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      t   t-1   t-2   t-3   t-4   t-5   t-6   t-7   t-8   t-9  t-10  month  \\\n",
       "0  30.5  41.5  62.5  70.5  69.5  67.0  61.0  60.5  73.0  81.0  92.0      1   \n",
       "1  26.0  30.5  41.5  62.5  70.5  69.5  67.0  61.0  60.5  73.0  81.0      1   \n",
       "2  28.5  26.0  30.5  41.5  62.5  70.5  69.5  67.0  61.0  60.5  73.0      1   \n",
       "3  28.0  28.5  26.0  30.5  41.5  62.5  70.5  69.5  67.0  61.0  60.5      1   \n",
       "4  32.5  28.0  28.5  26.0  30.5  41.5  62.5  70.5  69.5  67.0  61.0      1   \n",
       "\n",
       "   day  hour  dayofyear  weekofyear  dayofweek  quarter  season  \n",
       "0    1    11          1           1          1        1       1  \n",
       "1    1    12          1           1          1        1       1  \n",
       "2    1    13          1           1          1        1       1  \n",
       "3    1    14          1           1          1        1       1  \n",
       "4    1    15          1           1          1        1       1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfh = get_pm25_data_for_modelling('ml', 'h')\n",
    "dfh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96378, 19)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Limit training data to 2 last years of hourly data (for performance reasons)\n",
    "df = dfh[:]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>t-1</th>\n",
       "      <th>t-2</th>\n",
       "      <th>t-3</th>\n",
       "      <th>t-4</th>\n",
       "      <th>t-5</th>\n",
       "      <th>t-6</th>\n",
       "      <th>t-7</th>\n",
       "      <th>t-8</th>\n",
       "      <th>t-9</th>\n",
       "      <th>t-10</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>dayofyear</th>\n",
       "      <th>weekofyear</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>quarter</th>\n",
       "      <th>season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30.5</td>\n",
       "      <td>41.5</td>\n",
       "      <td>62.5</td>\n",
       "      <td>70.5</td>\n",
       "      <td>69.5</td>\n",
       "      <td>67.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>60.5</td>\n",
       "      <td>73.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26.0</td>\n",
       "      <td>30.5</td>\n",
       "      <td>41.5</td>\n",
       "      <td>62.5</td>\n",
       "      <td>70.5</td>\n",
       "      <td>69.5</td>\n",
       "      <td>67.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>60.5</td>\n",
       "      <td>73.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28.5</td>\n",
       "      <td>26.0</td>\n",
       "      <td>30.5</td>\n",
       "      <td>41.5</td>\n",
       "      <td>62.5</td>\n",
       "      <td>70.5</td>\n",
       "      <td>69.5</td>\n",
       "      <td>67.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>60.5</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28.0</td>\n",
       "      <td>28.5</td>\n",
       "      <td>26.0</td>\n",
       "      <td>30.5</td>\n",
       "      <td>41.5</td>\n",
       "      <td>62.5</td>\n",
       "      <td>70.5</td>\n",
       "      <td>69.5</td>\n",
       "      <td>67.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>60.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32.5</td>\n",
       "      <td>28.0</td>\n",
       "      <td>28.5</td>\n",
       "      <td>26.0</td>\n",
       "      <td>30.5</td>\n",
       "      <td>41.5</td>\n",
       "      <td>62.5</td>\n",
       "      <td>70.5</td>\n",
       "      <td>69.5</td>\n",
       "      <td>67.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      t   t-1   t-2   t-3   t-4   t-5   t-6   t-7   t-8   t-9  t-10  month  \\\n",
       "0  30.5  41.5  62.5  70.5  69.5  67.0  61.0  60.5  73.0  81.0  92.0      1   \n",
       "1  26.0  30.5  41.5  62.5  70.5  69.5  67.0  61.0  60.5  73.0  81.0      1   \n",
       "2  28.5  26.0  30.5  41.5  62.5  70.5  69.5  67.0  61.0  60.5  73.0      1   \n",
       "3  28.0  28.5  26.0  30.5  41.5  62.5  70.5  69.5  67.0  61.0  60.5      1   \n",
       "4  32.5  28.0  28.5  26.0  30.5  41.5  62.5  70.5  69.5  67.0  61.0      1   \n",
       "\n",
       "   day  hour  dayofyear  weekofyear  dayofweek  quarter  season  \n",
       "0    1    11          1           1          1        1       1  \n",
       "1    1    12          1           1          1        1       1  \n",
       "2    1    13          1           1          1        1       1  \n",
       "3    1    14          1           1          1        1       1  \n",
       "4    1    15          1           1          1        1       1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='model_h'></a>\n",
    "\n",
    "## Base Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define first past/future cutoff point in time offset (1 year of data)\n",
    "cut_off_offset = 365*24 # for hourly data\n",
    "#cut_off_offset = 365 # for daily data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this method to work properly, observations in the data frame must be ordered by time (the greater index, the recent data)\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = split_df_for_ml_modelling_offset(data=df, target_col='t', cut_off_offset=cut_off_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((87618, 18), (87618,), (8760, 18), (8760,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('LinearRegression', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False))\n",
      "('ElasticNet', ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
      "           max_iter=1000, normalize=False, positive=False, precompute=False,\n",
      "           random_state=None, selection='cyclic', tol=0.0001, warm_start=False))\n",
      "('SVR', SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='scale',\n",
      "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False))\n",
      "('DecisionTreeRegressor', DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=None,\n",
      "                      max_features=None, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=None, splitter='best'))\n",
      "('KNeighborsRegressor', KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                    metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                    weights='uniform'))\n",
      "('AdaBoostRegressor', AdaBoostRegressor(base_estimator=None, learning_rate=1.0, loss='linear',\n",
      "                  n_estimators=50, random_state=None))\n",
      "('BaggingRegressor', BaggingRegressor(base_estimator=None, bootstrap=True, bootstrap_features=False,\n",
      "                 max_features=1.0, max_samples=1.0, n_estimators=10,\n",
      "                 n_jobs=None, oob_score=False, random_state=None, verbose=0,\n",
      "                 warm_start=False))\n",
      "('RandomForestRegressor', RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=10, n_jobs=None, oob_score=False,\n",
      "                      random_state=None, verbose=0, warm_start=False))\n",
      "('ExtraTreesRegressor', ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
      "                    max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                    max_samples=None, min_impurity_decrease=0.0,\n",
      "                    min_impurity_split=None, min_samples_leaf=1,\n",
      "                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                    n_estimators=10, n_jobs=None, oob_score=False,\n",
      "                    random_state=None, verbose=0, warm_start=False))\n"
     ]
    }
   ],
   "source": [
    "# Define linear and non-linear regression models in scope\n",
    "reg_models = get_models_for_regression()\n",
    "models = []\n",
    "\n",
    "for model in reg_models:\n",
    "    item = (type(model).__name__, model)\n",
    "    models.append(item)\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ml_metrics.py | 36 | score_ml_models | 12-Jun-20 22:13:26 | INFO: Scoring LinearRegression started\n",
      "ml_metrics.py | 36 | score_ml_models | 12-Jun-20 22:13:26 | INFO: Scoring ElasticNet started\n",
      "ml_metrics.py | 36 | score_ml_models | 12-Jun-20 22:13:26 | INFO: Scoring SVR started\n",
      "ml_metrics.py | 36 | score_ml_models | 12-Jun-20 22:31:15 | INFO: Scoring DecisionTreeRegressor started\n",
      "ml_metrics.py | 36 | score_ml_models | 12-Jun-20 22:31:20 | INFO: Scoring KNeighborsRegressor started\n",
      "ml_metrics.py | 36 | score_ml_models | 12-Jun-20 22:31:26 | INFO: Scoring AdaBoostRegressor started\n",
      "ml_metrics.py | 36 | score_ml_models | 12-Jun-20 22:31:47 | INFO: Scoring BaggingRegressor started\n",
      "ml_metrics.py | 36 | score_ml_models | 12-Jun-20 22:32:19 | INFO: Scoring RandomForestRegressor started\n",
      "ml_metrics.py | 36 | score_ml_models | 12-Jun-20 22:32:49 | INFO: Scoring ExtraTreesRegressor started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression, RMSE 8.454518896700154, (std. dev. 1.0553480373018875)\n",
      "ElasticNet, RMSE 8.466827600091218, (std. dev. 1.067885147340825)\n",
      "SVR, RMSE 11.326321281986267, (std. dev. 3.2873896157758)\n",
      "DecisionTreeRegressor, RMSE 11.935730744140105, (std. dev. 1.212603561790327)\n",
      "KNeighborsRegressor, RMSE 10.866634500393763, (std. dev. 1.608640951710884)\n",
      "AdaBoostRegressor, RMSE 16.822330057803956, (std. dev. 2.3545417407828704)\n",
      "BaggingRegressor, RMSE 8.826528266269595, (std. dev. 1.1106023504757678)\n",
      "RandomForestRegressor, RMSE 8.805528004207913, (std. dev. 1.118664501467662)\n",
      "ExtraTreesRegressor, RMSE 8.818717598661783, (std. dev. 1.1022531965880644)\n",
      "CPU times: user 19min 39s, sys: 4 s, total: 19min 43s\n",
      "Wall time: 19min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Perform initial ranking - choose best performing models for validation\n",
    "# We assume, models will predict the best t+1, for subsequent ts the error will grow. That is why, for the initial ranking we will use one day prediction only\n",
    "scores, results, names = score_ml_models(X_train=X_train,\n",
    "                                         y_train=y_train,\n",
    "                                         models=models,\n",
    "                                         n_splits = 5,\n",
    "                                         metric='neg_root_mean_squared_error',\n",
    "                                         metric_label=\"RMSE\", \n",
    "                                         seed=123)\n",
    "for score in scores:\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearRegression, RMSE 8.454518896700154, (std. dev. 1.0553480373018875)\n",
    "ElasticNet, RMSE 8.466827600091218, (std. dev. 1.067885147340825)\n",
    "SVR, RMSE 11.326321281986267, (std. dev. 3.2873896157758)\n",
    "DecisionTreeRegressor, RMSE 11.935730744140105, (std. dev. 1.212603561790327)\n",
    "KNeighborsRegressor, RMSE 10.866634500393763, (std. dev. 1.608640951710884)\n",
    "AdaBoostRegressor, RMSE 16.822330057803956, (std. dev. 2.3545417407828704)\n",
    "BaggingRegressor, RMSE 8.826528266269595, (std. dev. 1.1106023504757678)\n",
    "RandomForestRegressor, RMSE 8.805528004207913, (std. dev. 1.118664501467662)\n",
    "ExtraTreesRegressor, RMSE 8.818717598661783, (std. dev. 1.1022531965880644)\n",
    "CPU times: user 19min 39s, sys: 4 s, total: 19min 43s\n",
    "Wall time: 19min 40s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare Algorithms\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "fig.suptitle('Algorithms Comparison - RMSE')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.savefig('images/ml_comparison_rmse_h.png')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='model_h_tune'></a>\n",
    "\n",
    "## Hyper-parameter Tuning\n",
    "\n",
    "`Hyper-parameters` are parameters that are not directly learnt within estimators. It is possible and recommended to search the hyper-parameter space **for the best cross validation score**.\n",
    "\n",
    "While using a grid of parameter settings is currently the most widely used method for parameter optimization (`GridSearchCV`), other search methods have more favourable properties. `RandomizedSearchCV` implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values. This has two main benefits over an exhaustive search:\n",
    "\n",
    "- A budget can be chosen independent of the number of parameters and possible values.\n",
    "- Adding parameters that do not influence the performance does not decrease efficiency.\n",
    "\n",
    "Specifying how parameters should be sampled is done using a dictionary, very similar to specifying parameters for `GridSearchCV`. Additionally, a computation budget, being the number of sampled candidates or sampling iterations, is specified using the n_iter parameter. For each parameter, either a distribution over possible values or a list of discrete choices (which will be sampled uniformly) can be specified: loguniform(1, 100) can be used instead of [1, 10, 100]. \n",
    "\n",
    "https://towardsdatascience.com/hyperparameter-tuning-c5619e7e6624\n",
    "\n",
    "-----\n",
    "\n",
    "## Grid Search: hyper-parameters tuning\n",
    "https://scikit-learn.org/stable/modules/grid_search.html#grid-search\n",
    "\n",
    "Hyper-parameters are parameters that are not directly learnt within estimators. In scikit-learn they are passed as arguments to the constructor of the estimator classes. Typical examples include C, kernel and gamma for Support Vector Classifier, alpha for Lasso, etc.\n",
    "\n",
    "Some models allow for specialized, efficient parameter search strategies, outlined below. Two generic approaches to sampling search candidates are provided in scikit-learn: for given values, GridSearchCV exhaustively considers all parameter combinations, while RandomizedSearchCV can sample a given number of candidates from a parameter space with a specified distribution.\n",
    "\n",
    "Note that it is common that a small subset of those parameters can have a large impact on the predictive or computation performance of the model while others can be left to their default values. It is recommended to read the docstring of the estimator class to get a finer understanding of their expected behavior, possibly by reading the enclosed reference to the literature.\n",
    "\n",
    "While using a grid of parameter settings is currently the most widely used method for parameter optimization, other search methods have more favourable properties. RandomizedSearchCV implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values. This has two main benefits over an exhaustive search:\n",
    "\n",
    "- A budget can be chosen independent of the number of parameters and possible values.\n",
    "\n",
    "- Adding parameters that do not influence the performance does not decrease efficiency.\n",
    "\n",
    "## Cross-validation: evaluating estimator performance\n",
    "https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "\n",
    "Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called overfitting. To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a test set X_test, y_test.\n",
    "\n",
    "When evaluating different settings (“hyperparameters”) for estimators, such as the C setting that must be manually set for an SVM, there is still a risk of overfitting on the test set because the parameters can be tweaked until the estimator performs optimally. This way, knowledge about the test set can “leak” into the model and evaluation metrics no longer report on generalization performance. To solve this problem, yet another part of the dataset can be held out as a so-called “validation set”: training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set.\n",
    "\n",
    "However, by partitioning the available data into three sets, we drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets.\n",
    "\n",
    "A solution to this problem is a procedure called cross-validation (CV for short). A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. In the basic approach, called k-fold CV, the training set is split into k smaller sets (other approaches are described below, but generally follow the same principles). The following procedure is followed for each of the k “folds”:\n",
    "\n",
    "- A model is trained using  of the folds as training data;\n",
    "\n",
    "- the resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).\n",
    "\n",
    "The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop. This approach can be computationally expensive, but does not waste too much data (as is the case when fixing an arbitrary validation set), which is a major advantage in problems such as inverse inference where the number of samples is very small.\n",
    "\n",
    "dac obrazek ze strony, zrobic source: sklearn\n",
    "\n",
    "## Pipelines and composite estimators\n",
    "https://scikit-learn.org/stable/modules/compose.html\n",
    "\n",
    "Transformers are usually combined with classifiers, regressors or other estimators to build a composite estimator. The most common tool is a Pipeline.\n",
    "\n",
    "Pipeline can be used to chain multiple estimators into one. This is useful as there is often a fixed sequence of steps in processing the data, for example feature selection, normalization and classification. Pipeline serves multiple purposes here:\n",
    "\n",
    "Convenience and encapsulation\n",
    "You only have to call fit and predict once on your data to fit a whole sequence of estimators.\n",
    "\n",
    "Joint parameter selection\n",
    "You can grid search over parameters of all estimators in the pipeline at once.\n",
    "\n",
    "Safety\n",
    "Pipelines help avoid leaking statistics from your test data into the trained model in cross-validation, by ensuring that the same samples are used to train the transformers and predictors.\n",
    "\n",
    "All estimators in a pipeline, except the last one, must be transformers (i.e. must have a transform method). The last estimator may be any type (transformer, classifier, etc.).\n",
    "\n",
    "## Model persistence\n",
    "https://scikit-learn.org/stable/modules/model_persistence.html\n",
    "\n",
    "After training a scikit-learn model, it is desirable to have a way to persist the model for future use without having to retrain. The following section gives you an example of how to persist a model with pickle. \n",
    "\n",
    "pickle (and joblib by extension), has some issues regarding maintainability and security. Because of this,\n",
    "\n",
    "- Never unpickle untrusted data as it could lead to malicious code being executed upon loading.\n",
    "\n",
    "- While models saved using one version of scikit-learn might load in other versions, this is entirely unsupported and inadvisable. It should also be kept in mind that operations performed on such data could give different and unexpected results.\n",
    "\n",
    "In order to rebuild a similar model with future versions of scikit-learn, additional metadata should be saved along the pickled model:\n",
    "\n",
    "- The training data, e.g. a reference to an immutable snapshot\n",
    "\n",
    "- The python source code used to generate the model\n",
    "\n",
    "- The versions of scikit-learn and its dependencies\n",
    "\n",
    "- The cross validation score obtained on the training data\n",
    "\n",
    "- This should make it possible to check that the cross-validation score is in the same range as before.\n",
    "\n",
    "Since a model internal representation may be different on two different architectures, dumping a model on one architecture and loading it on another architecture is not supported.\n",
    "\n",
    "more: https://pyvideo.org/pycon-us-2014/pickles-are-for-delis-not-software.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the best median RMSE metric results, the following algorithms seem to work best with PM2.5 data (cutoff < 8.5 g/m3) and the best non-linear algorithm:\n",
    "- LinearRegression\n",
    "- ElasticNet\n",
    "- RandomForestRegressor\n",
    "\n",
    "After hyper-parameter tuning, they will be validated using a common procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('LinearRegression',\n",
       " dict_keys(['copy_X', 'fit_intercept', 'n_jobs', 'normalize']))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[0][0], models[0][1].get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 7.87 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# LinearRegression\n",
    "param_grid = {\n",
    "    \"fit_intercept\": [True, False],\n",
    "    \"normalize\": [True, False],\n",
    "    \"n_jobs\": [-1]\n",
    "}\n",
    "model = models[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regression.py | 70 | perform_grid_search_cv | 12-Jun-20 22:36:53 | INFO: Best: -8.417609654057864 using {'fit_intercept': True, 'n_jobs': -1, 'normalize': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.01 s, sys: 1.09 s, total: 6.1 s\n",
      "Wall time: 925 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "perform_grid_search_cv(X_train=X_train, \n",
    "                        y_train=y_train, \n",
    "                        model=model, \n",
    "                        param_grid=param_grid, \n",
    "                        scoring='neg_root_mean_squared_error', \n",
    "                        num_folds=6, \n",
    "                        seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression.py | 70 | perform_grid_search_cv | 12-Jun-20 22:36:53 | INFO: Best: -8.417609654057864 using {'fit_intercept': True, 'n_jobs': -1, 'normalize': True}\n",
    "CPU times: user 5.01 s, sys: 1.09 s, total: 6.1 s\n",
    "Wall time: 925 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ElasticNet',\n",
       " dict_keys(['alpha', 'copy_X', 'fit_intercept', 'l1_ratio', 'max_iter', 'normalize', 'positive', 'precompute', 'random_state', 'selection', 'tol', 'warm_start']))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[1][0], models[1][1].get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17 µs, sys: 3 µs, total: 20 µs\n",
      "Wall time: 24.8 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ElasticNet\n",
    "param_grid = {\n",
    "    \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    \"l1_ratio\": np.arange(0.0, 1.0, 0.1),\n",
    "    \"max_iter\": [1, 10, 100, 500, 1000],\n",
    "    \"fit_intercept\": [True, False],\n",
    "    \"selection\": [\"cyclic\", \"random\"]\n",
    "}\n",
    "model = models[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regression.py | 70 | perform_grid_search_cv | 12-Jun-20 23:03:26 | INFO: Best: -8.420425074217084 using {'alpha': 0.001, 'fit_intercept': True, 'l1_ratio': 0.9, 'max_iter': 500, 'selection': 'random'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 40min 6s, sys: 13min 37s, total: 1h 53min 44s\n",
      "Wall time: 23min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "perform_grid_search_cv(X_train=X_train, \n",
    "                        y_train=y_train, \n",
    "                        model=model, \n",
    "                        param_grid=param_grid, \n",
    "                        scoring='neg_root_mean_squared_error', \n",
    "                        num_folds=6, \n",
    "                        seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression.py | 70 | perform_grid_search_cv | 12-Jun-20 23:03:26 | INFO: Best: -8.420425074217084 using {'alpha': 0.001, 'fit_intercept': True, 'l1_ratio': 0.9, 'max_iter': 500, 'selection': 'random'}\n",
    "CPU times: user 1h 40min 6s, sys: 13min 37s, total: 1h 53min 44s\n",
    "Wall time: 23min 35s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('RandomForestRegressor',\n",
       " dict_keys(['bootstrap', 'ccp_alpha', 'criterion', 'max_depth', 'max_features', 'max_leaf_nodes', 'max_samples', 'min_impurity_decrease', 'min_impurity_split', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', 'n_estimators', 'n_jobs', 'oob_score', 'random_state', 'verbose', 'warm_start']))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[7][0], models[7][1].get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 µs, sys: 0 ns, total: 8 µs\n",
      "Wall time: 12.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# RandomForestRegressor\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "# A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset \n",
    "# and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the max_samples \n",
    "# parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree.\n",
    "param_grid = {\n",
    "    \"criterion\": ['mse', 'mae'],\n",
    "    \"min_samples_split\": [2, 8, 16],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"n_estimators\": [100, 200],\n",
    "    \"n_jobs\": [-1]\n",
    "}\n",
    "model = models[7][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Random search, to speed-up\n",
    "perform_random_search_cv(X_train=X_train, \n",
    "                        y_train=y_train, \n",
    "                        model=model, \n",
    "                        param_grid=param_grid, \n",
    "                        scoring='neg_root_mean_squared_error', \n",
    "                        num_folds=6, \n",
    "                        seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate best models on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = [\n",
    "    LinearRegression({'fit_intercept': True, 'n_jobs': -1, 'normalize': True}),\n",
    "    ElasticNet(**XXXX{'alpha': 0.001, 'fit_intercept': True, 'l1_ratio': 0.2, 'max_iter': 10, 'selection': 'random'}),\n",
    "    RandomForestRegressor(**XXXX{'n_jobs': -1, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 4, 'criterion': 'mae'}),\n",
    "]\n",
    "\n",
    "models = []\n",
    "\n",
    "for model in best_models:\n",
    "    item = (type(model).__name__, model)\n",
    "    models.append(item)\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of t-n columns in a dataset\n",
    "n_lags_in_dataset = 10 # for hourly data\n",
    "#n_lags_in_dataset = 4 # for daily data\n",
    "\n",
    "# Set datetime format for index\n",
    "dt_format = \"%Y-%m-%d %H:%M:%S\" # for hourly data\n",
    "#dt_format = \"%Y-%m-%d\" # for daily data\n",
    "\n",
    "# Create train and validate sets\n",
    "train_test_split_position = int(len(df)-cut_off_offset)\n",
    "\n",
    "# Create as many folds as remains till the end of known data\n",
    "n_folds = len(df) #train_test_split_position+3\n",
    "\n",
    "# Predict for X points\n",
    "n_pred_points = 24 # for hourly data\n",
    "#n_pred_points = 7 # for daily data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Validate result on test\n",
    "# Creates 365*24*24 models for hourly data, or 365*7 models for hourly data\n",
    "  \n",
    "model = models[0][1]\n",
    "\n",
    "fold_results = walk_forward_ml_model_validation(data=df,\n",
    "                                                 model=model,\n",
    "                                                 target_col='t',\n",
    "                                                 cut_off_offset=cut_off_offset,\n",
    "                                                 n_pred_points=n_pred_points,\n",
    "                                                 n_folds=-1,\n",
    "                                                 n_lags = n_lags_in_dataset)\n",
    "print(len(fold_results))\n",
    "print(fold_results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialize output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "model_name = models[0][0]\n",
    "\n",
    "timestamp = get_datetime_identifier(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "path = f'results/pm25_ml_{model_name}_results_h_{timestamp}.joblib'\n",
    "\n",
    "dump(fold_results, path) \n",
    "fold_results = load(path)\n",
    "print(len(fold_results))\n",
    "print(fold_results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Returns a list of mean folds RMSE for n_pred_points (starting at 1 point forecast)\n",
    "res = get_mean_folds_rmse_for_n_prediction_points(fold_results=fold_results, n_pred_points=n_pred_points)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show forecasts for n-th point in the future\n",
    "show_n_points_of_forecasts = [1, 12, 24] # for hourly data\n",
    "#show_n_points_of_forecasts = [1, 3, 7] # for daily data\n",
    "\n",
    "# Used to zoom the plots (date ranges shown in the plots)\n",
    "# for hourly data\n",
    "start_end_dates = [('2018-01-01', '2019-01-01'), ('2018-02-01', '2018-03-01'), ('2018-06-01', '2018-07-01')]\n",
    "# for daily data\n",
    "#start_end_dates = [('2018-01-01', '2019-01-01'), ('2018-02-01', '2018-04-01'), ('2018-06-01', '2018-08-01')]\n",
    "\n",
    "# Type of plot\n",
    "# 0 -> plot_observed_vs_predicted\n",
    "# 1 -> plot_observed_vs_predicted_with_error\n",
    "plot_types = [0, 1, 1]\n",
    "\n",
    "# File names for plots (format png will be used, do not add .png extension)\n",
    "base_file_path = f'images/pm25_obs_vs_pred_365_h_ts_{model_name}' # for hourly data\n",
    "#base_file_path = f'images/pm25_obs_vs_pred_365_d_ml_{model_name}' # for daily data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_results[0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to convert to datetime index for plotting\n",
    "\n",
    "# Replace integer-based index with datetime-based index\n",
    "\n",
    "# We remove n_lags_in_dataset, b/c when the dataset with t-n lags was created\n",
    "# rows conatining NaNs were removed\n",
    "df_datetime = get_pm25_data_for_modelling('ts', 'd')[n_lags_in_dataset:]\n",
    "#df_datetime.head()\n",
    "#df_datetime[0:1].index.strftime(dt_format)[0]\n",
    "\n",
    "# Integer to datetime index using a mapper (ts version of the dataset)\n",
    "for i in range(0, len(fold_results)):\n",
    "    if not isinstance(fold_results[i].index, pd.DatetimeIndex):\n",
    "        # Create a table of datetime indexes corresponding to integer indexes\n",
    "        datetime_indexes = []\n",
    "        integer_indexes = fold_results[i].index\n",
    "        for ix in integer_indexes:\n",
    "            datetime_indexes.append(df_datetime[ix:ix+1].index.strftime(dt_format)[0])\n",
    "        print(datetime_indexes)\n",
    "        \n",
    "        # Apply datetime_indexes as index for fold data\n",
    "        fold_results[i].index = pd.to_datetime(datetime_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_results[0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results(show_n_points_of_forecasts=show_n_points_of_forecasts,\n",
    "                   start_end_dates=start_end_dates,\n",
    "                   plot_types=plot_types,\n",
    "                   base_file_path=base_file_path,\n",
    "                   fold_results=fold_results, \n",
    "                   n_pred_points=n_pred_points, \n",
    "                   cut_off_offset=cut_off_offset, \n",
    "                   model_name=model_name,\n",
    "                timestamp=timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#LinearRegression\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html\n",
    "# Linear regression with combined L1 and L2 priors as regularizer.\n",
    "param_grid = {\n",
    "    \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    \"l1_ratio\": np.arange(0.0, 1.0, 0.1),\n",
    "    \"max_iter\": [1, 10, 100, 1000],\n",
    "    \"fit_intercept\": [True, False],\n",
    "    \"selection\": [\"cyclic\", \"random\"]\n",
    "}\n",
    "model = models[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_grid_search_cv2(X_train=X_train, \n",
    "                        y_train=y_train, \n",
    "                        model=model, \n",
    "                        param_grid=param_grid, \n",
    "                        scoring='neg_root_mean_squared_error', \n",
    "                        num_folds=6, \n",
    "                        seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best: -5.559832349894503 using {'alpha': 0.1, 'fit_intercept': True, 'l1_ratio': 0.9, 'max_iter': 10, 'selection': 'cyclic'}\n",
    "CPU times: user 8min 30s, sys: 3min 59s, total: 12min 30s\n",
    "Wall time: 2min 13s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#LinearRegression - Random Search\n",
    "perform_random_search_cv2(X_train=X_train, \n",
    "                        y_train=y_train, \n",
    "                        model=model, \n",
    "                        param_grid=param_grid, \n",
    "                        scoring='neg_root_mean_squared_error', \n",
    "                        num_folds=6, \n",
    "                        seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fitting 5 folds for each of 800 candidates, totalling 4000 fits\n",
    "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
    "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.1s\n",
    "[Parallel(n_jobs=-1)]: Done 976 tasks      | elapsed:   11.4s\n",
    "[Parallel(n_jobs=-1)]: Done 2458 tasks      | elapsed:   27.9s\n",
    "[Parallel(n_jobs=-1)]: Done 2984 tasks      | elapsed:   34.8s\n",
    "Best: -5.564184376166167 using {'selection': 'cyclic', 'max_iter': 10, 'l1_ratio': 0.9, 'fit_intercept': True, 'alpha': 0.1}\n",
    "CPU times: user 10.8 s, sys: 1.5 s, total: 12.3 s\n",
    "Wall time: 40.9 s\n",
    "[Parallel(n_jobs=-1)]: Done 3985 out of 4000 | elapsed:   40.7s remaining:    0.2s\n",
    "[Parallel(n_jobs=-1)]: Done 4000 out of 4000 | elapsed:   40.8s finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#ElasticNet\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html\n",
    "# Linear regression with combined L1 and L2 priors as regularizer.\n",
    "param_grid = {\n",
    "    \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    \"l1_ratio\": np.arange(0.0, 1.0, 0.1),\n",
    "    \"max_iter\": [1, 10, 100, 1000],\n",
    "    \"fit_intercept\": [True, False],\n",
    "    \"selection\": [\"cyclic\", \"random\"]\n",
    "}\n",
    "model = models[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_grid_search_cv2(X_train=X_train, \n",
    "                        y_train=y_train, \n",
    "                        model=model, \n",
    "                        param_grid=param_grid, \n",
    "                        scoring='neg_root_mean_squared_error', \n",
    "                        num_folds=6, \n",
    "                        seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best: -5.559832349894503 using {'alpha': 0.1, 'fit_intercept': True, 'l1_ratio': 0.9, 'max_iter': 10, 'selection': 'cyclic'}\n",
    "CPU times: user 8min 53s, sys: 4min 26s, total: 13min 20s\n",
    "Wall time: 2min 33s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#ElasticNet - Random Search\n",
    "perform_random_search_cv2(X_train=X_train, \n",
    "                        y_train=y_train, \n",
    "                        model=model, \n",
    "                        param_grid=param_grid, \n",
    "                        scoring='neg_root_mean_squared_error', \n",
    "                        num_folds=6, \n",
    "                        seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fitting 5 folds for each of 800 candidates, totalling 4000 fits\n",
    "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
    "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.0s\n",
    "[Parallel(n_jobs=-1)]: Done 560 tasks      | elapsed:    7.2s\n",
    "[Parallel(n_jobs=-1)]: Done 1146 tasks      | elapsed:   22.4s\n",
    "[Parallel(n_jobs=-1)]: Done 1632 tasks      | elapsed:   34.2s\n",
    "[Parallel(n_jobs=-1)]: Done 3000 tasks      | elapsed:   48.2s\n",
    "Best: -5.5639353632257675 using {'selection': 'random', 'max_iter': 100, 'l1_ratio': 0.7000000000000001, 'fit_intercept': True, 'alpha': 0.1}\n",
    "CPU times: user 14.9 s, sys: 1.61 s, total: 16.5 s\n",
    "Wall time: 56.4 s\n",
    "[Parallel(n_jobs=-1)]: Done 4000 out of 4000 | elapsed:   56.4s finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluded\n",
    "%%time\n",
    "#DecisionTreeRegressor\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n",
    "# Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n",
    "param_grid = {\n",
    "    \"criterion\": [\"mse\", \"friedman_mse\", \"mae\"],\n",
    "    \"splitter\": [\"best\", \"random\"],\n",
    "    \"max_depth\": range(2, 16, 2),\n",
    "    \"min_samples_split\": range(2, 16, 2),\n",
    "    \"min_samples_leaf\": range(2, 16, 2),\n",
    "    \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "    \"ccp_alpha\": [0.001, 0.01, 0.1, 0, 1, 10, 100, 1000]\n",
    "}\n",
    "model = models[3][1]\n",
    "perform_grid_search_cv2(X_train=X_train, \n",
    "                        y_train=y_train, \n",
    "                        model=model, \n",
    "                        param_grid=param_grid, \n",
    "                        scoring='neg_root_mean_squared_error', \n",
    "                        num_folds=6, \n",
    "                        seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#BaggingRegressor\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html\n",
    "# https://www.programcreek.com/python/example/85938/sklearn.ensemble.BaggingRegressor\n",
    "# A Bagging regressor is an ensemble meta-estimator that fits base regressors each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.\n",
    "# The base estimator to fit on random subsets of the dataset. If None, then the base estimator is a decision tree.\n",
    "param_grid = {\n",
    "    \"base_estimator\": [models[0][1]],\n",
    "    \"n_estimators\": [100, 200, 500, 1000, 5000, 10000],\n",
    "    \"bootstrap\": [True, False],\n",
    "    \"n_jobs\": [-1]\n",
    "}\n",
    "model = models[6][1]\n",
    "perform_grid_search_cv2(X_train=X_train, \n",
    "                        y_train=y_train, \n",
    "                        model=model, \n",
    "                        param_grid=param_grid, \n",
    "                        scoring='neg_root_mean_squared_error', \n",
    "                        num_folds=6, \n",
    "                        seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best: -5.563871725667823 using {'base_estimator': LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False), 'bootstrap': True, 'n_estimators': 500, 'n_jobs': -1}\n",
    "CPU times: user 1min 10s, sys: 1.73 s, total: 1min 11s\n",
    "Wall time: 6min 13s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#BaggingRegressor - Random Search\n",
    "perform_random_search_cv2(X_train=X_train, \n",
    "                        y_train=y_train, \n",
    "                        model=model, \n",
    "                        param_grid=param_grid, \n",
    "                        scoring='neg_root_mean_squared_error', \n",
    "                        num_folds=6, \n",
    "                        seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
    "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
    "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.3min\n",
    "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  4.1min finished\n",
    "Best: -5.566746325950773 using {'n_jobs': -1, 'n_estimators': 200, 'bootstrap': True, 'base_estimator': LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)}\n",
    "CPU times: user 443 ms, sys: 46.1 ms, total: 489 ms\n",
    "Wall time: 4min 5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#RandomForestRegressor\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "# https://www.programcreek.com/python/example/85938/sklearn.ensemble.BaggingRegressor\n",
    "# A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree.\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200, 500],#, 1000, 5000, 10000],\n",
    "    #\"criterion\": [\"mse\", \"mae\"],\n",
    "    #\"bootstrap\": [True, False],\n",
    "    \"ccp_alpha\": [0.001, 0.01, 0.1, 0, 1, 10],#, 100, 1000],\n",
    "    \"max_depth\": [10, 20, 30, 40, None],#50, 60, 70, 80, 90, 100, None],\n",
    "    \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    #\"min_samples_split\": range(2, 16, 2),\n",
    "    \"n_jobs\": [-1]\n",
    "}\n",
    "model = models[7][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "perform_grid_search_cv2(X_train=X_train, \n",
    "                        y_train=y_train, \n",
    "                        model=model, \n",
    "                        param_grid=param_grid, \n",
    "                        scoring='neg_root_mean_squared_error', \n",
    "                        num_folds=6, \n",
    "                        seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#RandomForestRegressor - Random Search\n",
    "perform_random_search_cv2(X_train=X_train, \n",
    "                        y_train=y_train, \n",
    "                        model=model, \n",
    "                        param_grid=param_grid, \n",
    "                        scoring='neg_root_mean_squared_error', \n",
    "                        num_folds=6, \n",
    "                        seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fitting 5 folds for each of 810 candidates, totalling 4050 fits\n",
    "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
    "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.5min\n",
    "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  9.2min\n",
    "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 30.5min\n",
    "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed: 46.8min\n",
    "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed: 78.1min\n",
    "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed: 109.5min\n",
    "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed: 139.0min\n",
    "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed: 175.4min\n",
    "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed: 229.0min\n",
    "[Parallel(n_jobs=-1)]: Done 4050 out of 4050 | elapsed: 229.4min finished\n",
    "Best: -5.611958006937581 using {'n_jobs': -1, 'n_estimators': 500, 'min_samples_leaf': 4, 'max_features': 'auto', 'max_depth': 10, 'ccp_alpha': 0}\n",
    "CPU times: user 1min 3s, sys: 4.94 s, total: 1min 8s\n",
    "Wall time: 3h 49min 30s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#ExtraTreesRegressor\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html\n",
    "# https://www.programcreek.com/python/example/102434/sklearn.ensemble.ExtraTreesRegressor\n",
    "# This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200, 500],#, 1000, 5000, 10000],\n",
    "    #\"criterion\": [\"mse\", \"mae\"],\n",
    "    #\"bootstrap\": [True, False],\n",
    "    \"ccp_alpha\": [0.001, 0.01, 0.1, 0, 1, 10],#, 100, 1000],\n",
    "    \"max_depth\": [10, 20, 30, 40, None],#50, 60, 70, 80, 90, 100, None],\n",
    "    #\"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    #\"min_samples_split\": range(2, 16, 2),\n",
    "    \"n_jobs\": [-1]\n",
    "}\n",
    "model = models[8][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_grid_search_cv2(X_train=X_train, \n",
    "                        y_train=y_train, \n",
    "                        model=model, \n",
    "                        param_grid=param_grid, \n",
    "                        scoring='neg_root_mean_squared_error', \n",
    "                        num_folds=6, \n",
    "                        seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#ExtraTreesRegressor - Random Search\n",
    "perform_random_search_cv2(X_train=X_train, \n",
    "                        y_train=y_train, \n",
    "                        model=model, \n",
    "                        param_grid=param_grid, \n",
    "                        scoring='neg_root_mean_squared_error', \n",
    "                        num_folds=6, \n",
    "                        seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
    "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
    "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   20.4s\n",
    "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed: 11.3min\n",
    "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 33.8min\n",
    "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed: 58.9min\n",
    "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed: 93.3min\n",
    "[Parallel(n_jobs=-1)]: Done 1350 out of 1350 | elapsed: 107.5min finished\n",
    "Best: -5.550630838028553 using {'n_jobs': -1, 'n_estimators': 500, 'min_samples_leaf': 4, 'max_depth': 20, 'ccp_alpha': 0}\n",
    "CPU times: user 23.8 s, sys: 2.48 s, total: 26.3 s\n",
    "Wall time: 1h 47min 34s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate best models on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = [\n",
    "    XXXLinearRegression({'selection': 'cyclic', 'max_iter': 10, 'l1_ratio': 0.9, 'fit_intercept': True, 'alpha': 0.1}),\n",
    "    XXXElasticNet(**{'alpha': 0.1, 'fit_intercept': True, 'l1_ratio': 0.9, 'max_iter': 10, 'selection': 'cyclic'}),\n",
    "    XXXBaggingRegressor(**{'base_estimator': LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False), 'bootstrap': True, 'n_estimators': 500, 'n_jobs': -1}),\n",
    "    XXXRandomForestRegressor(**{'n_jobs': -1, 'n_estimators': 500, 'min_samples_leaf': 4, 'max_features': 'auto', 'max_depth': 10, 'ccp_alpha': 0}),\n",
    "    XXXExtraTreesRegressor(**{'n_jobs': -1, 'n_estimators': 500, 'min_samples_leaf': 4, 'max_depth': 20, 'ccp_alpha': 0})\n",
    "]\n",
    "\n",
    "models = []\n",
    "\n",
    "for model in best_models:\n",
    "    item = (type(model).__name__, model)\n",
    "    models.append(item)\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, results, names = score_ml_models(X_train=X_test,\n",
    "                                         y_train=y_test,\n",
    "                                         models=models,\n",
    "                                         n_splits = 5,\n",
    "                                         metric='neg_root_mean_squared_error',\n",
    "                                         metric_label=\"RMSE\", \n",
    "                                         seed=123)\n",
    "\n",
    "for score in scores:\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='data_d'></a>\n",
    "\n",
    "## Load daily data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd = get_pm25_data_for_modelling('ml', 'd')\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='model_d'></a>\n",
    "\n",
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dfd.copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define first past/future cutoff point in time offset (1 year of data)\n",
    "#cut_off_offset = 365*24 # for hourly data\n",
    "cut_off_offset = 365 # for daily data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this method to work properly, observations in the data frame must be ordered by time (the greater index, the recent data)\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = split_df_for_ml_modelling_offset(data=df, target_col='t', cut_off_offset=cut_off_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define linear and non-linear regression models in scope\n",
    "reg_models = get_models_for_regression()\n",
    "models = []\n",
    "\n",
    "for model in reg_models:\n",
    "    item = (type(model).__name__, model)\n",
    "    models.append(item)\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Perform initial ranking - choose best performing models for validation\n",
    "# We assume, models will predict the best t+1, for subsequent ts the error will grow. That is why, for the initial ranking we will use one day prediction only\n",
    "scores, results, names = score_ml_models(X_train=X_train,\n",
    "                                         y_train=y_train,\n",
    "                                         models=models,\n",
    "                                         n_splits = 5,\n",
    "                                         metric='neg_root_mean_squared_error',\n",
    "                                         metric_label=\"RMSE\", \n",
    "                                         seed=123)\n",
    "for score in scores:\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearRegression, RMSE 21.71519880662688, (std. dev. 2.9273799166199206)\n",
    "ElasticNet, RMSE 21.830784202119624, (std. dev. 2.9573842361855087)\n",
    "SVR, RMSE 24.045745977510276, (std. dev. 4.441156440786244)\n",
    "DecisionTreeRegressor, RMSE 31.211049027394875, (std. dev. 2.9718767572307927)\n",
    "KNeighborsRegressor, RMSE 23.707382569743043, (std. dev. 3.3448167988995414)\n",
    "AdaBoostRegressor, RMSE 26.824657640640293, (std. dev. 3.330626762800735)\n",
    "BaggingRegressor, RMSE 23.32161001848838, (std. dev. 3.0897291100698667)\n",
    "RandomForestRegressor, RMSE 23.097728743053413, (std. dev. 3.101989348167566)\n",
    "ExtraTreesRegressor, RMSE 23.91481608230075, (std. dev. 3.0918830774859245)\n",
    "CPU times: user 4.03 s, sys: 112 ms, total: 4.14 s\n",
    "Wall time: 3.74 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Algorithms\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "fig.suptitle('Algorithms Comparison - RMSE')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.savefig('images/ml_comparison_rmse_d.png')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='model_d_tune'></a>\n",
    "\n",
    "## Hyper-parameter Tuning\n",
    "Based on the best median RMSE metric results, the following algorithms seem to work best with PM2.5 data (cutoff < 22.5 g/m3) and the best non-linear algorithm:\n",
    "- LinearRegression\n",
    "- ElasticNet\n",
    "- RandomForestRegressor\n",
    "\n",
    "After hyper-parameter tuning, they will be validated using a common procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[0][0], models[0][1].get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# LinearRegression\n",
    "param_grid = {\n",
    "    \"fit_intercept\": [True, False],\n",
    "    \"normalize\": [True, False],\n",
    "    \"n_jobs\": [-1]\n",
    "}\n",
    "model = models[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "perform_grid_search_cv(X_train=X_train, \n",
    "                        y_train=y_train, \n",
    "                        model=model, \n",
    "                        param_grid=param_grid, \n",
    "                        scoring='neg_root_mean_squared_error', \n",
    "                        num_folds=6, \n",
    "                        seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best: -21.68400804843603 using {'fit_intercept': True, 'n_jobs': -1, 'normalize': False}\n",
    "CPU times: user 221 ms, sys: 28.4 ms, total: 249 ms\n",
    "Wall time: 89.8 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[1][0], models[1][1].get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# ElasticNet\n",
    "param_grid = {\n",
    "    \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    \"l1_ratio\": np.arange(0.0, 1.0, 0.1),\n",
    "    \"max_iter\": [1, 10, 100, 500, 1000],\n",
    "    \"fit_intercept\": [True, False],\n",
    "    \"selection\": [\"cyclic\", \"random\"]\n",
    "}\n",
    "model = models[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "perform_grid_search_cv(X_train=X_train, \n",
    "                        y_train=y_train, \n",
    "                        model=model, \n",
    "                        param_grid=param_grid, \n",
    "                        scoring='neg_root_mean_squared_error', \n",
    "                        num_folds=6, \n",
    "                        seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best: -21.67493314318791 using {'alpha': 0.001, 'fit_intercept': True, 'l1_ratio': 0.2, 'max_iter': 10, 'selection': 'random'}\n",
    "CPU times: user 2min 19s, sys: 15.8 s, total: 2min 35s\n",
    "Wall time: 39.3 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[7][0], models[7][1].get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#RandomForestRegressor\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "# A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset \n",
    "# and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the max_samples \n",
    "# parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree.\n",
    "param_grid = {\n",
    "    \"criterion\": ['mse', 'mae'],\n",
    "    \"min_samples_split\": [2, 8, 16],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"n_estimators\": [100, 200],\n",
    "    \"n_jobs\": [-1]\n",
    "}\n",
    "model = models[7][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Random, to speed-up\n",
    "#RandomForestRegressor - Random Search\n",
    "perform_random_search_cv(X_train=X_train, \n",
    "                        y_train=y_train, \n",
    "                        model=model, \n",
    "                        param_grid=param_grid, \n",
    "                        scoring='neg_root_mean_squared_error', \n",
    "                        num_folds=6, \n",
    "                        seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best: -21.76997790505864 using {'n_jobs': -1, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 4, 'criterion': 'mae'}\n",
    "CPU times: user 1min 22s, sys: 931 ms, total: 1min 23s\n",
    "Wall time: 17min 42s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate best models on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = [\n",
    "    LinearRegression({'fit_intercept': True, 'n_jobs': -1, 'normalize': False}),\n",
    "    ElasticNet(**{'alpha': 0.001, 'fit_intercept': True, 'l1_ratio': 0.2, 'max_iter': 10, 'selection': 'random'}),\n",
    "    RandomForestRegressor(**{'n_jobs': -1, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 4, 'criterion': 'mae'}),\n",
    "]\n",
    "\n",
    "models = []\n",
    "\n",
    "for model in best_models:\n",
    "    item = (type(model).__name__, model)\n",
    "    models.append(item)\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of t-n columns in a dataset\n",
    "#n_lags_in_dataset = 10 # for hourly data\n",
    "n_lags_in_dataset = 4 # for daily data\n",
    "\n",
    "# Set datetime format for index\n",
    "#dt_format = \"%Y-%m-%d %H:%M:%S\" # for hourly data\n",
    "dt_format = \"%Y-%m-%d\" # for daily data\n",
    "\n",
    "# Create train and validate sets\n",
    "train_test_split_position = int(len(df)-cut_off_offset)\n",
    "\n",
    "# Create as many folds as remains till the end of known data\n",
    "n_folds = len(df) #train_test_split_position+3\n",
    "\n",
    "# Predict for X points\n",
    "#n_pred_points = 24 # for hourly data\n",
    "n_pred_points = 7 # for daily data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Validate result on test\n",
    "# Creates 365*24*24 models for hourly data, or 365*7 models for hourly data\n",
    "  \n",
    "# Model 1\n",
    "model_name = models[0][0]\n",
    "model = models[0][1]\n",
    "\n",
    "# Model 2\n",
    "\n",
    "\n",
    "# Model 3\n",
    "\n",
    "fold_results = walk_forward_ml_model_validation(data=df,\n",
    "                                                 model=model,\n",
    "                                                 target_col='t',\n",
    "                                                 cut_off_offset=cut_off_offset,\n",
    "                                                 n_pred_points=n_pred_points,\n",
    "                                                 n_folds=-1,\n",
    "                                                 n_lags = n_lags_in_dataset)\n",
    "print(len(fold_results))\n",
    "print(fold_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "358\n",
    "       observed  predicted      error  abs_error\n",
    "3650  67.991848  56.841447  11.150400  11.150400\n",
    "3651  16.026950  61.386935  45.359985  45.359985\n",
    "3652  14.590020  23.431795   8.841774   8.841774\n",
    "3653  22.094854  31.352299   9.257445   9.257445\n",
    "3654  62.504217  33.263056  29.241160  29.241160\n",
    "3655  43.929804  59.087193  15.157389  15.157389\n",
    "3656  22.088192  43.298562  21.210370  21.210370\n",
    "CPU times: user 45.7 s, sys: 4.33 s, total: 50 s\n",
    "Wall time: 16.9 s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialize output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "timestamp = get_datetime_identifier(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "path = f'results/pm25_ml_{model_name}_results_d_{timestamp}.joblib'\n",
    "\n",
    "dump(fold_results, path) \n",
    "fold_results = load(path)\n",
    "print(len(fold_results))\n",
    "print(fold_results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Returns a list of mean folds RMSE for n_pred_points (starting at 1 point forecast)\n",
    "res = get_mean_folds_rmse_for_n_prediction_points(fold_results=fold_results, n_pred_points=n_pred_points)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "('LinearRegression', LinearRegression(copy_X=True,\n",
    "                 fit_intercept={'fit_intercept': True, 'n_jobs': -1,\n",
    "                                'normalize': False},\n",
    "                 n_jobs=None, normalize=False))\n",
    "\n",
    "[9.462926210826211, 9.488959544159545, 9.400442735042732, 9.458610541310541, 9.551721652421653, 9.50755527065527, 9.564808262108263]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show forecasts for n-th point in the future\n",
    "#show_n_points_of_forecasts = [1, 12, 24] # for hourly data\n",
    "show_n_points_of_forecasts = [1, 3, 7] # for daily data\n",
    "\n",
    "# Used to zoom the plots (date ranges shown in the plots)\n",
    "# for hourly data\n",
    "#start_end_dates = [('2018-01-01', '2019-01-01'), ('2018-02-01', '2018-03-01'), ('2018-06-01', '2018-07-01')]\n",
    "# for daily data\n",
    "start_end_dates = [('2018-01-01', '2019-01-01'), ('2018-02-01', '2018-04-01'), ('2018-06-01', '2018-08-01')]\n",
    "\n",
    "# Type of plot\n",
    "# 0 -> plot_observed_vs_predicted\n",
    "# 1 -> plot_observed_vs_predicted_with_error\n",
    "plot_types = [0, 1, 1]\n",
    "\n",
    "# File names for plots (format png will be used, do not add .png extension)\n",
    "#base_file_path = f'images/pm25_obs_vs_pred_365_h_ts_{model_name}' # for hourly data\n",
    "base_file_path = f'images/pm25_obs_vs_pred_365_d_ml_{model_name}' # for daily data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_results[0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to convert to datetime index for plotting\n",
    "\n",
    "# Replace integer-based index with datetime-based index\n",
    "\n",
    "# We remove n_lags_in_dataset, b/c when the dataset with t-n lags was created\n",
    "# rows conatining NaNs were removed\n",
    "df_datetime = get_pm25_data_for_modelling('ts', 'd')[n_lags_in_dataset:]\n",
    "#df_datetime.head()\n",
    "#df_datetime[0:1].index.strftime(dt_format)[0]\n",
    "\n",
    "# Integer to datetime index using a mapper (ts version of the dataset)\n",
    "for i in range(0, len(fold_results)):\n",
    "    if not isinstance(fold_results[i].index, pd.DatetimeIndex):\n",
    "        # Create a table of datetime indexes corresponding to integer indexes\n",
    "        datetime_indexes = []\n",
    "        integer_indexes = fold_results[i].index\n",
    "        for ix in integer_indexes:\n",
    "            datetime_indexes.append(df_datetime[ix:ix+1].index.strftime(dt_format)[0])\n",
    "        #print(datetime_indexes)\n",
    "        \n",
    "        # Apply datetime_indexes as index for fold data\n",
    "        fold_results[i].index = pd.to_datetime(datetime_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_results[0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results(show_n_points_of_forecasts=show_n_points_of_forecasts,\n",
    "                   start_end_dates=start_end_dates,\n",
    "                   plot_types=plot_types,\n",
    "                   base_file_path=base_file_path,\n",
    "                   fold_results=fold_results, \n",
    "                   n_pred_points=n_pred_points, \n",
    "                   cut_off_offset=cut_off_offset, \n",
    "                   model_name=model_name,\n",
    "                timestamp=timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "dump(model, 'model.joblib') \n",
    "model2 = load('model.joblib')\n",
    "model2.predict(X[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "timestamp = get_datetime_identifier(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "for model in models:\n",
    "    path = f'results/pm25_ml_{model[0]}_model_d_{timestamp}.joblib'\n",
    "    dump(model, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "path = f'results/pm25_ml_RandomForestRegressor_model_d_2020-06-12_18-06-21.joblib'\n",
    "model2 = load(path)\n",
    "print(model2[0], model2[1])\n",
    "model2_fitted = model2[1].fit(X_train, y_train)\n",
    "print(model2_fitted.predict(X_test[0:1]))\n",
    "print(y_test[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForestRegressor RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mae',\n",
    "                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "                      max_samples=None, min_impurity_decrease=0.0,\n",
    "                      min_impurity_split=None, min_samples_leaf=4,\n",
    "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                      n_estimators=100, n_jobs=-1, oob_score=False,\n",
    "                      random_state=None, verbose=0, warm_start=False)\n",
    "[74.06882094]\n",
    "3650    67.991848\n",
    "Name: t, dtype: float64\n",
    "CPU times: user 46.5 s, sys: 191 ms, total: 46.7 s\n",
    "Wall time: 7.12 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
